{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fejE7Jez7wY"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLo5gvC90Sef"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC6P5gYT0h26"
      },
      "source": [
        "# ðŸ“˜ Table of Contents\n",
        "\n",
        "1. [ðŸ”§ Import Libraries](#-import-libraries)\n",
        "2. [ðŸš€ Initialize Resume Screening System](#-initialize-resume-screening-system)\n",
        "3. [ðŸ“¤ Resume Upload and Text Extraction](#-resume-upload-and-text-extraction)\n",
        "4. [ðŸ§  Embedding Generation & FAISS Indexing](#-embedding-generation--faiss-indexing)\n",
        "5. [ðŸ“ Resume Scoring & Evaluation](#-resume-scoring--evaluation)\n",
        "6. [ðŸ“Š Ranking, Fit Score & Recommendation](#-ranking-fit-score--recommendation)\n",
        "7. [ðŸ“ Exporting Results & Summarization](#-exporting-results--summarization)\n",
        "8. [ðŸ’¬ Interactive Resume Q&A](#-interactive-resume-qa)\n",
        "9. [âš ï¸ Skill Gap Detection](#-skill-gap-detection)\n",
        "10. [ðŸ” Resume Similarity & Plagiarism](#-resume-similarity--plagiarism)\n",
        "11. [ðŸ“§ Recruiter Notification & Alerts](#-recruiter-notification--alerts)\n",
        "12. [ðŸ“† Interview Scheduling with Google Calendar](#-interview-scheduling-with-google-calendar)\n",
        "13. [ðŸ”— LinkedIn Profile Extraction](#-linkedin-profile-extraction)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrXNRFAgGCYs"
      },
      "source": [
        "# ðŸ§  Ultra-Clean Resume Screening System  \n",
        "This notebook implements all 22 resume screening features using GPT-4o, FAISS, hybrid scoring, and a full RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMSjXoA90vpC"
      },
      "source": [
        "## ðŸ”§ Install Libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T-4fj4ib4FW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY1K858cVraq"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# ===============================\n",
        "# âœ… INSTALL ALL REQUIRED PACKAGES (ONCE)\n",
        "# ===============================\n",
        "!pip install -q faiss-cpu pdfplumber pytesseract python-docx fpdf \\\n",
        "sentence-transformers langchain langchain-community pymupdf \\\n",
        "langchain-openai openai matplotlib pycountry\n",
        "\n",
        "# âœ… INSTALL SYSTEM DEPENDENCIES FOR OCR SUPPORT\n",
        "!apt-get install -y poppler-utils tesseract-ocr > /dev/null 2>&1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g73EkBpF8wA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_BuqMxg0zvR"
      },
      "source": [
        "## ðŸ”§ Install  & Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2BXTH6sVvDz",
        "outputId": "87b1c333-a5dc-4af2-b911-8cb45d949e14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.28)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.72)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.97.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.72)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===============================\n",
        "# âœ… INSTALL ALL REQUIRED PACKAGES\n",
        "# ===============================\n",
        "!pip install --upgrade openai\n",
        "\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q pdfplumber pytesseract\n",
        "!pip install pymupdf\n",
        "!pip install -q langchain langchain-community\n",
        "!pip install langchain-openai\n",
        "!pip install -U langchain-community\n",
        "!pip install --upgrade langchain\n",
        "\n",
        "!apt-get install -y poppler-utils tesseract-ocr > /dev/null 2>&1\n",
        "!pip install -q langchain fpdf python-docx sentence-transformers\n",
        "\n",
        "# ===============================\n",
        "# âœ… FINAL CLEAN IMPORT BLOCK (NO DUPLICATES)\n",
        "# ===============================\n",
        "\n",
        "# Core\n",
        "import os, json, re, numpy as np, pandas as pd\n",
        "\n",
        "# NLP & ML\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# PDF & OCR\n",
        "import pdfplumber, pytesseract, fitz  # fitz = PyMuPDF\n",
        "from PIL import Image\n",
        "from fpdf import FPDF\n",
        "from docx import Document\n",
        "from io import BytesIO\n",
        "\n",
        "# Visuals\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# OpenAI & LangChain\n",
        "\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "\n",
        "from langchain_community import chat_models  # If needed internally\n",
        "\n",
        "\n",
        "# System + Uploads\n",
        "import pycountry\n",
        "\n",
        "\n",
        "# Optional: Colab-friendly display tools\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbyq0U6Pw56p"
      },
      "source": [
        "## âœ… Initialize OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIV0_H5OkMLi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# âœ… Initialize OpenAI client\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# âœ… Set your OpenAI key securely\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb0E4_0P26-b"
      },
      "source": [
        "## ðŸš€ Initialize Resume Screening System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGRpRmpai-2p"
      },
      "outputs": [],
      "source": [
        "# âœ… STEP 1: Define your class first\n",
        "\n",
        "class ResumeScreeningSystem:\n",
        "    def __init__(self, embed_model='all-mpnet-base-v2', llm_model='gpt-4o'):\n",
        "        self.model = SentenceTransformer(embed_model)\n",
        "        self.client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "        self.util = util\n",
        "        self.faiss = faiss\n",
        "        self.re = re\n",
        "        self.pd = pd\n",
        "        self.TfidfVectorizer = TfidfVectorizer\n",
        "        self.cosine_similarity = cosine_similarity\n",
        "        self.resume_texts = {}\n",
        "        self.resume_embeddings = []\n",
        "        self.resume_names = []\n",
        "        self.index = None\n",
        "        self.tfidf = None\n",
        "        self.tfidf_matrix = None\n",
        "        self.job_description = \"\"\n",
        "        self.job_embedding = None\n",
        "        self.LLM_MODEL = llm_model\n",
        "        self.latest_results = []\n",
        "\n",
        " # Resume Upload and Text Extraction\n",
        "\n",
        "    def extract_text(self, file, filename=\"uploaded_file\"):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            try:\n",
        "                with pdfplumber.open(file) as pdf:\n",
        "                    return \"\\n\".join([p.extract_text() for p in pdf.pages if p.extract_text()])\n",
        "            except:\n",
        "                return self.extract_with_ocr(file)\n",
        "        elif file.name.endswith(\".docx\"):\n",
        "            return \"\\n\".join([p.text for p in Document(file).paragraphs]) # Fixed: Corrected the list comprehension\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "\n",
        "    def extract_with_ocr(self, file):\n",
        "        try:\n",
        "            pdf = fitz.open(stream=file.read(), filetype=\"pdf\")\n",
        "            text = []\n",
        "            for page in pdf:\n",
        "                img = Image.open(BytesIO(page.get_pixmap().tobytes()))\n",
        "                text.append(pytesseract.image_to_string(img))\n",
        "            return \"\\n\".join(text)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ OCR failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "     # Embedding Generation & FAISS Indexing\n",
        "\n",
        "    def generate_resume_embeddings(self):\n",
        "        self.resume_names = list(self.resume_texts.keys())\n",
        "        self.resume_embeddings = [self.model.encode(text) for text in self.resume_texts.values()]\n",
        "\n",
        "    def build_faiss_index(self):\n",
        "        self.generate_resume_embeddings()\n",
        "        if not self.resume_embeddings:\n",
        "           raise ValueError(\"Resume embeddings not generated yet.\")\n",
        "        dim = len(self.resume_embeddings[0])\n",
        "        self.index = self.faiss.IndexFlatIP(dim)  # Cosine similarity via inner product\n",
        "\n",
        "            # âœ… Normalize resume embeddings (crucial for cosine similarity)\n",
        "        self.resume_embeddings = [v / np.linalg.norm(v) for v in self.resume_embeddings]\n",
        "        self.index.add(np.array(self.resume_embeddings))  # âœ… Add normalized vectors to FAISS\n",
        "\n",
        "\n",
        "\n",
        "    def build_tfidf_matrix(self):\n",
        "        texts = list(self.resume_texts.values()) + [self.job_description]\n",
        "        self.tfidf = self.TfidfVectorizer(stop_words='english')\n",
        "        self.tfidf_matrix = self.tfidf.fit_transform(texts)\n",
        "\n",
        "         # âœ… Save FAISS index + resume data\n",
        "    def save_faiss_index(self, index_path=\"faiss_index.index\"):\n",
        "        if self.index:\n",
        "            faiss.write_index(self.index, index_path)\n",
        "            np.save(\"resume_embeddings.npy\", self.resume_embeddings)\n",
        "            with open(\"resume_names.json\", \"w\") as f:\n",
        "                 json.dump(self.resume_names, f)\n",
        "            print(\"âœ… FAISS index and metadata saved.\")\n",
        "\n",
        "    # âœ… Load saved FAISS index + resume data\n",
        "    def load_faiss_index(self, index_path=\"faiss_index.index\"):\n",
        "        if os.path.exists(index_path):\n",
        "           self.index = faiss.read_index(index_path)\n",
        "           self.resume_embeddings = np.load(\"resume_embeddings.npy\", allow_pickle=True)\n",
        "           with open(\"resume_names.json\") as f:\n",
        "             self.resume_names = json.load(f)\n",
        "           print(\"âœ… FAISS index and metadata loaded.\")\n",
        "        else:\n",
        "           print(\"âš ï¸ No saved FAISS index found.\")\n",
        "\n",
        "\n",
        "    # âœ… Multi-job matching loop\n",
        "    def run_multi_job_evaluation_loop(self, job_list):\n",
        "        all_results = {}\n",
        "        for job in job_list:\n",
        "            print(f\"\\n============================\\nðŸ“Œ Job: {job}\")\n",
        "            self.job_description = job\n",
        "            results = self.evaluate_resumes()\n",
        "            top = sorted(results, key=lambda x: -x['Similarity'])[:5]\n",
        "            for r in top:\n",
        "                print(f\"- {r['Name']}: {r['Fit']} ({r['Similarity']})\")\n",
        "            all_results[job] = top\n",
        "        return all_results\n",
        "\n",
        "\n",
        "# Resume Scoring & Evaluation\n",
        "\n",
        "    def evaluate_resumes(self, semantic_weight=0.7):\n",
        "    # Step 1: Build indexes\n",
        "        self.build_faiss_index()\n",
        "        self.build_tfidf_matrix()\n",
        "\n",
        "    # Step 2: Encode the job description\n",
        "        self.job_embedding = self.model.encode(self.job_description)\n",
        "\n",
        "    # Step 3: Semantic similarity using FAISS (cosine)\n",
        "        normalized_job_embed = self.job_embedding / np.linalg.norm(self.job_embedding)\n",
        "        scores, _ = self.index.search(np.array([normalized_job_embed]), len(self.resume_embeddings))\n",
        "        semantic_sim = scores[0]\n",
        "\n",
        "\n",
        "    # Step 4: Keyword similarity using TF-IDF cosine\n",
        "        keyword_sim = self.cosine_similarity(\n",
        "        self.tfidf_matrix[-1],\n",
        "        self.tfidf_matrix[:-1]\n",
        "    ).flatten()\n",
        "\n",
        "    # Step 5: Combine both similarities using the semantic weight\n",
        "        hybrid = semantic_weight * semantic_sim + (1 - semantic_weight) * keyword_sim\n",
        "        hybrid = hybrid.tolist()  # âœ… Ensure hybrid is iterable\n",
        "\n",
        "    # Step 6: Initialize result container\n",
        "        self.latest_results = []\n",
        "\n",
        "    # Step 7: Loop through scores and evaluate\n",
        "        for i, score in enumerate(hybrid):\n",
        "            category = self.categorize_score(score)\n",
        "            print(f\"ðŸ“„ {self.resume_names[i]} â€“ Score: {score:.4f} â†’ {category}\")\n",
        "\n",
        "            self.latest_results.append({\n",
        "            \"Name\": self.resume_names[i],\n",
        "            \"Similarity\": round(score, 3),\n",
        "            \"Fit\": self.categorize_score(score),\n",
        "            \"Recommendation\": self.tag_highly_recommended([score])[0] # Fixed: tag_highly_recommended expects a list\n",
        "        })\n",
        "\n",
        "        return self.latest_results\n",
        "\n",
        "\n",
        "    def categorize_score(self, score):\n",
        "        if score >= 0.70:\n",
        "             return \"Excellent\"\n",
        "        elif score >= 0.55:\n",
        "              return \"Good\"\n",
        "        elif score >= 0.40:\n",
        "              return \"Fair\"\n",
        "        return \"Poor\"\n",
        "\n",
        "\n",
        "    def tag_highly_recommended(self, scores, threshold=0.80):\n",
        "        return [\"Highly Recommended\" if s >= threshold else \"Standard\" for s in scores]\n",
        "\n",
        "\n",
        "# Interactive Resume Q&A\n",
        "#RAG (Retrieval-Augmented Generation)\n",
        "# ðŸ†• Helper: Get top-N resume contexts for RAG\n",
        "\n",
        "\n",
        "    def get_top_resume_contexts(self, top_n=3):\n",
        "       if self.index is None:\n",
        "           raise ValueError(\"FAISS index not built yet.\")\n",
        "       if self.job_embedding is None or len(self.job_embedding) == 0:\n",
        "           self.job_embedding = self.model.encode(self.job_description)\n",
        "\n",
        "       scores, idx = self.index.search(np.array([self.job_embedding]), top_n)\n",
        "       context = \"\\n\\n\".join([self.resume_texts[self.resume_names[i]] for i in idx[0]])\n",
        "       return context\n",
        "\n",
        "\n",
        "\n",
        "    def run_gpt_prompt(self, prompt):\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(  # âœ… INVOKE the GPT model\n",
        "                model=self.LLM_MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "           )\n",
        "            return response.choices[0].message.content     # âœ… COMPLETION: get the model's reply\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ GPT Error: {e}\")\n",
        "            return \"âš ï¸ GPT failed to respond.\"\n",
        "\n",
        "\n",
        "\n",
        "    def rag_query_answering(self, query):\n",
        "        q_embed = self.model.encode(query)\n",
        "        scores, idx = self.index.search(np.array([q_embed]), 3)\n",
        "        context = \"\\n\\n\".join([self.resume_texts[self.resume_names[i]] for i in idx[0]])\n",
        "        prompt = f\"Answer based on resumes:\\n{context}\\n\\nQuestion: {query}\"\n",
        "        return self.run_gpt_prompt(prompt)\n",
        "\n",
        "\n",
        "    def rag_resume_similarity_analysis(self):\n",
        "        embs = self.model.encode(list(self.resume_texts.values()), convert_to_tensor=True)\n",
        "        sim_matrix = self.util.cos_sim(embs, embs).cpu().numpy()\n",
        "\n",
        "        names = list(self.resume_texts.keys())\n",
        "        results = []\n",
        "\n",
        "        for i in range(len(names)):\n",
        "            for j in range(i + 1, len(names)):  # avoid duplicate pairs\n",
        "                sim_score = sim_matrix[i][j]\n",
        "                percentage = round(float(sim_score) * 100, 2)\n",
        "                results.append((names[i], names[j], percentage))  # âœ… correct format\n",
        "\n",
        "        results.sort(key=lambda x: x[2], reverse=True)  # sort by similarity\n",
        "        return results  # âœ… returns list of tuples\n",
        "\n",
        "\n",
        "\n",
        "#####\n",
        "\n",
        "\n",
        "# âœ… Auto-improve weakest resumes using similarity (replaces manual version)\n",
        "    def rag_resume_improvement(self, dummy_input=\"\"):\n",
        "        try:\n",
        "            # Get top 3 strongest resumes based on FAISS similarity\n",
        "            top_scores, top_indices = self.index.search(np.array([self.job_embedding]), len(self.resume_names))\n",
        "            top_names = [self.resume_names[i] for i in top_indices[0][:3]]\n",
        "\n",
        "            # Exclude top resumes, sort others by similarity (lowest = weakest)\n",
        "            non_top = [r for r in self.latest_results if r[\"Name\"] not in top_names]\n",
        "            weak_resumes = sorted(non_top, key=lambda r: r[\"Similarity\"])[:2]  # 2 lowest-similarity resumes\n",
        "\n",
        "            # Get top resume text context for comparison\n",
        "            context = \"\\n\\n\".join([self.resume_texts[name] for name in top_names])\n",
        "\n",
        "            # Generate improvement suggestions\n",
        "            output = \"\"\n",
        "            for r in weak_resumes:\n",
        "                name = r[\"Name\"]\n",
        "                resume_text = self.resume_texts[name]\n",
        "                prompt = (\n",
        "                    f\"You are a professional resume coach. The job description is:\\n\\n\"\n",
        "                    f\"{self.job_description}\\n\\n\"\n",
        "                    f\"Here are 3 strong resumes:\\n{context}\\n\\n\"\n",
        "                    f\"Now suggest specific improvements for the following resume (Name: {name}):\\n\\n\"\n",
        "                    f\"{resume_text}\\n\\n\"\n",
        "                    f\"Guidelines:\\n\"\n",
        "                    f\"- Write 3â€“5 bullet points\\n\"\n",
        "                    f\"- Be specific and helpful\\n\"\n",
        "                    f\"- Focus on clarity, missing content, formatting, or skills\\n\"\n",
        "                )\n",
        "                result = self.run_gpt_prompt(prompt)\n",
        "                output += f\"\\nðŸ“„ {name} â€“ Suggested Improvements:\\n{result}\\n{'-'*80}\\n\"\n",
        "\n",
        "            return output or \"âš ï¸ No weak resumes found to improve.\"\n",
        "        except Exception as e:\n",
        "            return f\"âŒ RAG improvement failed: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# âœ… RAG: Detect Missing Skills (Compare with top resumes)\n",
        "\n",
        "\n",
        "    def detect_missing_skills(self):\n",
        "        try:\n",
        "            context = self.get_top_resume_contexts(top_n=3)  # ðŸ‘ˆ You can change to top_n=5 if needed\n",
        "            print(f\"ðŸ“„ Total resumes found: {len(self.resume_texts)}\")\n",
        "            output = \"\"\n",
        "            for name, resume_text in self.resume_texts.items():\n",
        "                prompt = (\n",
        "                    f\"ðŸ§  You are a highly precise resume screening expert.\\n\\n\"\n",
        "                    f\"ðŸŽ¯ TASK: Identify **only the clearly missing skills** in the target resume.\\n\"\n",
        "                    f\"DO NOT hallucinate or invent skills that are not explicitly mentioned in the job or top resumes.\\n\"\n",
        "                    f\"DO NOT guess or include generic soft skills.\\n\"\n",
        "                    f\"DO NOT explain anything.\\n\"\n",
        "                    f\"DO NOT include skills that are already present.\\n\"\n",
        "                    f\"Format the output exactly like below.\\n\\n\"\n",
        "                    f\"ðŸ“Œ Job Description:\\n{self.job_description}\\n\\n\"\n",
        "                    f\"ðŸ“Œ Top 3 Candidate Resumes:\\n{context}\\n\\n\"\n",
        "                    f\"ðŸ“„ Target Resume (Name: {name}):\\n{resume_text}\\n\\n\"\n",
        "                    f\"âœ… Output Format:\\n\"\n",
        "                    f\"{name} Resume â€“ Missing Skills: Skill1, Skill2, Skill3\"\n",
        "            )\n",
        "                result = self.run_gpt_prompt(prompt)\n",
        "                output += f\"\\nðŸ§¾ Analyzing Resume: {name}\\n{result}\\n\"\n",
        "            return output\n",
        "        except Exception as e:\n",
        "            return f\"âŒ RAG skill detection failed: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # âœ… Generate mock interview questions based on job, top resumes, and target resume\n",
        "    def generate_mock_questions(self, resume_text):\n",
        "        try:\n",
        "            context = self.get_top_resume_contexts(top_n=3)  # Top 3 candidates as context\n",
        "            prompt = (\n",
        "            \"You are a technical interviewer creating mock interview questions for a candidate.\\n\\n\"\n",
        "            \"Here is the job description:\\n\"\n",
        "            f\"{self.job_description}\\n\\n\"\n",
        "            \"Here are examples of top candidate resumes:\\n\"\n",
        "            f\"{context}\\n\\n\"\n",
        "            \"Here is the target candidate's resume:\\n\"\n",
        "            f\"{resume_text}\\n\\n\"\n",
        "            \"Based on the job requirements and top resumes, generate 5 strong mock interview questions tailored to this candidate. \"\n",
        "            \"Focus on skills, experience, and any competitive gaps.\\n\\n\"\n",
        "            \"Format your output as a numbered list.\"\n",
        "           )\n",
        "            return self.run_gpt_prompt(prompt)\n",
        "        except Exception as e:\n",
        "            return f\"âŒ Error generating mock questions: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "    # âœ… RAG: Recommend Job Roles\n",
        "\n",
        "    def recommend_roles(self, dummy_input=\"\"):\n",
        "        try:\n",
        "            import pandas as pd  # âœ… for Excel export\n",
        "\n",
        "            # âœ… Get top 3 resumes for RAG context\n",
        "            scores, indices = self.index.search(np.array([self.job_embedding]), len(self.resume_names))\n",
        "            top_indices = indices[0][:3]\n",
        "            top_names = [self.resume_names[i] for i in top_indices]\n",
        "            context = \"\\n\\n\".join([self.resume_texts[name] for name in top_names])\n",
        "\n",
        "           # âœ… Prepare a list to hold Excel data\n",
        "            excel_data = []\n",
        "\n",
        "           # âœ… Loop through ALL resumes\n",
        "            for name, resume_text in self.resume_texts.items():\n",
        "            # ðŸ”¢ Determine Level\n",
        "                if name in top_names:\n",
        "                 level = \"TOP\"\n",
        "                else:\n",
        "                     idx = self.resume_names.index(name)\n",
        "                     sim_score = scores[0][idx] * 100\n",
        "                     if sim_score >= 70:\n",
        "                        level = \"MID\"\n",
        "                     else:\n",
        "                        level = \"LOW\"\n",
        "\n",
        "            # âœ… GPT prompt for job roles\n",
        "            prompt = (\n",
        "                   f\"You are an expert career advisor. \"\n",
        "                   f\"Based on this resume, the job description, and the top resumes, \"\n",
        "                   f\"recommend exactly 3 job roles that best match the resumeâ€™s skills and experience.\\n\\n\"\n",
        "                   f\"Job Description:\\n{self.job_description}\\n\\n\"\n",
        "                   f\"Top Resumes:\\n{context}\\n\\n\"\n",
        "                   f\"Target Resume (Name: {name}):\\n{resume_text}\\n\\n\"\n",
        "                   f\"Return only 3 job titles, each on its own line.\"\n",
        "             )\n",
        "            result = self.run_gpt_prompt(prompt)\n",
        "\n",
        "            # âœ… Clean and split roles\n",
        "            roles = [r.strip(\"-â€¢ \\n\") for r in result.split(\"\\n\") if r.strip()]\n",
        "            while len(roles) < 3:  # ensure always 3 roles\n",
        "                roles.append(\"\")\n",
        "\n",
        "            role1, role2, role3 = roles[:3]\n",
        "\n",
        "            # âœ… Add to Excel data\n",
        "            excel_data.append({\n",
        "                \"Resume Name\": name,\n",
        "                \"Level\": level,\n",
        "                \"Role 1\": role1,\n",
        "                \"Role 2\": role2,\n",
        "                \"Role 3\": role3\n",
        "            })\n",
        "\n",
        "        # âœ… Convert to DataFrame\n",
        "            df = pd.DataFrame(excel_data)\n",
        "\n",
        "        # âœ… Save to Excel\n",
        "            excel_file = \"/content/job_role_recommendations.xlsx\"\n",
        "            df.to_excel(excel_file, index=False)\n",
        "\n",
        "            return f\"âœ… Excel exported: {excel_file}\\nðŸ“‚ Check the left sidebar to download.\"\n",
        "        except Exception as e:\n",
        "            return f\"âŒ RAG role recommendation failed: {e}\"\n",
        "\n",
        "\n",
        "##############################################################\n",
        "# Export & Summarization\n",
        "\n",
        "    def generate_resume_summary_pdf(self, top_n=3, filename=\"top_resumes.pdf\"):\n",
        "        top = sorted(self.latest_results, key=lambda x: -x['Similarity'])[:top_n]\n",
        "        pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", size=12)\n",
        "        for r in top:\n",
        "            summary = self.run_gpt_prompt(f\"Summarize this resume:\\n\\n{self.resume_texts[r['Name']]}\")\n",
        "            pdf.multi_cell(0, 10, f\"Name: {r['Name']}\\n\\nSummary:\\n{summary}\\n\\n---\\n\")\n",
        "        pdf.output(filename)\n",
        "\n",
        "    def export_resumes_to_pdf(self, results=None, filename=\"results.pdf\"):\n",
        "        if not results: results = self.latest_results\n",
        "        pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", size=12)\n",
        "        for r in results:\n",
        "            pdf.multi_cell(0, 10, f\"Name: {r['Name']}\\nScore: {r['Similarity']}\\nFit: {r['Fit']}\\n---\\n\")\n",
        "        pdf.output(filename)\n",
        "\n",
        "    def export_resumes_to_excel(self, results=None, filename=\"results.xlsx\"):\n",
        "        if not results: results = self.latest_results\n",
        "        self.pd.DataFrame(results).to_excel(filename, index=False)\n",
        "\n",
        "#   Analytics & Visualization\n",
        "\n",
        "    def show_dashboard(self):\n",
        "        df = self.pd.DataFrame(self.latest_results)\n",
        "        counts = df['Fit'].value_counts()\n",
        "        counts.plot(kind='bar')\n",
        "        plt.title(\"Resume Fit Distribution\")\n",
        "        plt.xlabel(\"Fit Category\")\n",
        "        plt.ylabel(\"Number of Resumes\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def generate_summary_pdf_for_top_n(self, top_n):\n",
        "\n",
        "\n",
        "        top_resumes = sorted(self.latest_results, key=lambda x: x.get('Similarity', 0), reverse=True)[:top_n]\n",
        "\n",
        "        if not top_resumes:\n",
        "            return \"âŒ No resume data found. Please evaluate resumes first.\"\n",
        "\n",
        "        pdf = FPDF()\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        pdf.cell(200, 10, txt=f\"Summary of Top {top_n} Resumes\", ln=True, align='C')\n",
        "        pdf.ln(5)\n",
        "\n",
        "        for i, resume in enumerate(top_resumes, start=1):\n",
        "            pdf.multi_cell(0, 10, txt=f\"\"\"\n",
        "{i}. Name: {resume['Name']}\n",
        "   Fit: {resume['Fit']}\n",
        "   Similarity: {resume['Similarity']}\n",
        "   Recommendation: {resume['Recommendation']}\n",
        "\"\"\")\n",
        "            pdf.ln(2)\n",
        "\n",
        "        output_path = \"/content/top_resume_summary.pdf\"\n",
        "        pdf.output(output_path)\n",
        "\n",
        "        return output_path\n",
        "\n",
        " # Recruiter Interaction\n",
        "    def generate_invite_email(self, name, job):\n",
        "        return self.run_gpt_prompt(f\"Write an interview invitation to {name} for {job} role.\")\n",
        "\n",
        "    def generate_job_description(self, title):\n",
        "        return self.run_gpt_prompt(f\"Write job description for: {title}\")\n",
        "\n",
        "\n",
        "    def auto_alert_recruiter(self, job_title=\"N/A\"):\n",
        "        top_resumes = [r for r in self.latest_results if r['Recommendation'] == \"Highly Recommended\"]\n",
        "        if not top_resumes:\n",
        "            print(\"ðŸ“­ No Highly Recommended resumes found.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\nðŸ“¢ --- Recruiter Alert ---\")\n",
        "        print(f\"ðŸ”” Top matching resumes found for job: {job_title}\")\n",
        "        print(f\"Number of top resumes: {len(top_resumes)}\")\n",
        "        print(\"Suggested action: Contact candidates or schedule interviews.\")\n",
        "        print(\"Names of top candidates:\")\n",
        "        for r in top_resumes:\n",
        "            print(f\"âœ… {r['Name']} â€” Fit: {r['Fit']}, Score: {r['Similarity']}\")\n",
        "        print(\"\\nðŸ“¬ Manually alert recruiter by email or internal system.\")\n",
        "\n",
        "    def schedule_interview(self, candidate_email, candidate_name, job_title, date_time_str):\n",
        "        subject = f\"Interview Invitation for {job_title}\"\n",
        "        message = self.generate_invite_email(candidate_name, job_title)\n",
        "\n",
        "        print(\"\\nðŸ“§ --- Interview Invitation Email ---\")\n",
        "        print(f\"To: {candidate_email}\")\n",
        "        print(f\"Subject: {subject}\")\n",
        "        print(\"Message:\\n\")\n",
        "        print(message)\n",
        "\n",
        "        print(\"\\nðŸ“… --- Calendar Event Info ---\")\n",
        "        print(f\"Event: Interview for {job_title}\")\n",
        "        print(f\"Candidate: {candidate_name}\")\n",
        "        print(f\"Date & Time (UTC): {date_time_str}\")\n",
        "        print(\"ðŸ“Œ Copy & paste this into your Gmail and Google Calendar manually.\")\n",
        "\n",
        " # Utility Functions\n",
        "\n",
        "    def extract_skill_profile(self, text):\n",
        "        return self.run_gpt_prompt(f\"List skills (tech + soft):\\n\\n{text}\")\n",
        "\n",
        "    def extract_contact_links(self, text):\n",
        "        email = self.re.findall(r\"[\\w.-]+@[\\w.-]+\", text)\n",
        "        linkedin = self.re.findall(r\"https?://(?:www\\.)?linkedin\\.com/in/[\\w-]+\", text)\n",
        "        github = self.re.findall(r\"https?://(?:www\\.)?github\\.com/[\\w-]+\", text)\n",
        "        return {\"emails\": list(set(email)), \"linkedin\": list(set(linkedin)), \"github\": list(set(github))}\n",
        "\n",
        "\n",
        "    def detect_country_from_text(self, text):\n",
        "        return list(set([c.name for c in pycountry.countries if c.name.lower() in text.lower()]))\n",
        "\n",
        "    def show_system_status(self):\n",
        "        print(f\"âœ… Resumes Loaded: {len(self.resume_texts)}\")\n",
        "        print(f\"âœ… Job Description Set: {'Yes' if self.job_description else 'No'}\")\n",
        "        print(f\"âœ… FAISS Index: {'Yes' if self.index else 'No'}\")\n",
        "        print(f\"âœ… Embeddings Ready: {'Yes' if self.resume_embeddings else 'No'}\")\n",
        "\n",
        "\n",
        "    ### cleanup command to allow re-running without kernel restart:\n",
        "\n",
        "    def reset_system(self):\n",
        "        self.resume_texts = {}\n",
        "        self.resume_embeddings = []\n",
        "        self.resume_names = []\n",
        "        self.index = None\n",
        "        self.tfidf = None\n",
        "        self.tfidf_matrix = None\n",
        "        self.latest_results = []\n",
        "        self.job_description = \"\"\n",
        "        self.job_embedding = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r7ClfXlki6S"
      },
      "source": [
        "## ðŸš€ Run & Evaluate the Resume Screening System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59RLFv0UDSov"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkFh3Q_SDSmy",
        "outputId": "ba73fb83-6547-40d2-f9f3-3b20d9caf54d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… GPT-4o Response: Hi there! How can I assist you today?\n",
            "ðŸ“‚ Processing: Adam John I.pdf ... âš ï¸ Empty content after extraction.\n",
            "ðŸ“‚ Processing: Jessica Thompson.pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: James  Patel .pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Maria Lopez.pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: John  Carter .pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Carlos Mendoza .pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Amanda Hughes.pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Emily Zhao.pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Isaac Roberts .pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Elizabeth Johnson.pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Marcus Fields.pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Michael Carter.pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Ethan Morales .pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: David Nguyen.pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Amanda Hughes - Copy.pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Laura Kim .pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Jonathan Lee.pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Daniel Evans.pdf ... âœ… Success\n",
            "ðŸ“‚ Processing: Ethan Clark.pdf ... âœ… Success\n",
            "\n",
            "============================\n",
            "ðŸ“Œ Job: AI Engineer with deployment experience\n",
            "ðŸ“„ Jessica Thompson.pdf â€“ Score: 0.4070 â†’ Fair\n",
            "ðŸ“„ James  Patel .pdf â€“ Score: 0.4210 â†’ Fair\n",
            "ðŸ“„ Maria Lopez.pdf â€“ Score: 0.4529 â†’ Fair\n",
            "ðŸ“„ John  Carter .pdf â€“ Score: 0.3948 â†’ Poor\n",
            "ðŸ“„ Carlos Mendoza .pdf â€“ Score: 0.3787 â†’ Poor\n",
            "ðŸ“„ Amanda Hughes.pdf â€“ Score: 0.3516 â†’ Poor\n",
            "ðŸ“„ Emily Zhao.pdf â€“ Score: 0.3190 â†’ Poor\n",
            "ðŸ“„ Isaac Roberts .pdf â€“ Score: 0.3307 â†’ Poor\n",
            "ðŸ“„ Elizabeth Johnson.pdf â€“ Score: 0.3238 â†’ Poor\n",
            "ðŸ“„ Marcus Fields.pdf â€“ Score: 0.3030 â†’ Poor\n",
            "ðŸ“„ Michael Carter.pdf â€“ Score: 0.2674 â†’ Poor\n",
            "ðŸ“„ Ethan Morales .pdf â€“ Score: 0.2359 â†’ Poor\n",
            "ðŸ“„ David Nguyen.pdf â€“ Score: 0.2325 â†’ Poor\n",
            "ðŸ“„ Amanda Hughes - Copy.pdf â€“ Score: 0.2335 â†’ Poor\n",
            "ðŸ“„ Laura Kim .pdf â€“ Score: 0.2327 â†’ Poor\n",
            "ðŸ“„ Jonathan Lee.pdf â€“ Score: 0.2239 â†’ Poor\n",
            "ðŸ“„ Daniel Evans.pdf â€“ Score: 0.2358 â†’ Poor\n",
            "ðŸ“„ Ethan Clark.pdf â€“ Score: 0.1833 â†’ Poor\n",
            "- Maria Lopez.pdf: Fair (0.453)\n",
            "- James  Patel .pdf: Fair (0.421)\n",
            "- Jessica Thompson.pdf: Fair (0.407)\n",
            "- John  Carter .pdf: Poor (0.395)\n",
            "- Carlos Mendoza .pdf: Poor (0.379)\n",
            "\n",
            "============================\n",
            "ðŸ“Œ Job: Data Scientist with Python and ML\n",
            "ðŸ“„ Jessica Thompson.pdf â€“ Score: 0.4150 â†’ Fair\n",
            "ðŸ“„ James  Patel .pdf â€“ Score: 0.3544 â†’ Poor\n",
            "ðŸ“„ Maria Lopez.pdf â€“ Score: 0.3782 â†’ Poor\n",
            "ðŸ“„ John  Carter .pdf â€“ Score: 0.3435 â†’ Poor\n",
            "ðŸ“„ Carlos Mendoza .pdf â€“ Score: 0.3624 â†’ Poor\n",
            "ðŸ“„ Amanda Hughes.pdf â€“ Score: 0.3752 â†’ Poor\n",
            "ðŸ“„ Emily Zhao.pdf â€“ Score: 0.3409 â†’ Poor\n",
            "ðŸ“„ Isaac Roberts .pdf â€“ Score: 0.3254 â†’ Poor\n",
            "ðŸ“„ Elizabeth Johnson.pdf â€“ Score: 0.3321 â†’ Poor\n",
            "ðŸ“„ Marcus Fields.pdf â€“ Score: 0.2874 â†’ Poor\n",
            "ðŸ“„ Michael Carter.pdf â€“ Score: 0.3571 â†’ Poor\n",
            "ðŸ“„ Ethan Morales .pdf â€“ Score: 0.2854 â†’ Poor\n",
            "ðŸ“„ David Nguyen.pdf â€“ Score: 0.2859 â†’ Poor\n",
            "ðŸ“„ Amanda Hughes - Copy.pdf â€“ Score: 0.2794 â†’ Poor\n",
            "ðŸ“„ Laura Kim .pdf â€“ Score: 0.2496 â†’ Poor\n",
            "ðŸ“„ Jonathan Lee.pdf â€“ Score: 0.2600 â†’ Poor\n",
            "ðŸ“„ Daniel Evans.pdf â€“ Score: 0.2389 â†’ Poor\n",
            "ðŸ“„ Ethan Clark.pdf â€“ Score: 0.1858 â†’ Poor\n",
            "- Jessica Thompson.pdf: Fair (0.415)\n",
            "- Maria Lopez.pdf: Poor (0.378)\n",
            "- Amanda Hughes.pdf: Poor (0.375)\n",
            "- Carlos Mendoza .pdf: Poor (0.362)\n",
            "- Michael Carter.pdf: Poor (0.357)\n",
            "\n",
            "============================\n",
            "ðŸ“Œ Job: NLP Researcher in Arabic Language\n",
            "ðŸ“„ Jessica Thompson.pdf â€“ Score: 0.2929 â†’ Poor\n",
            "ðŸ“„ James  Patel .pdf â€“ Score: 0.2634 â†’ Poor\n",
            "ðŸ“„ Maria Lopez.pdf â€“ Score: 0.2302 â†’ Poor\n",
            "ðŸ“„ John  Carter .pdf â€“ Score: 0.2155 â†’ Poor\n",
            "ðŸ“„ Carlos Mendoza .pdf â€“ Score: 0.2174 â†’ Poor\n",
            "ðŸ“„ Amanda Hughes.pdf â€“ Score: 0.1829 â†’ Poor\n",
            "ðŸ“„ Emily Zhao.pdf â€“ Score: 0.1734 â†’ Poor\n",
            "ðŸ“„ Isaac Roberts .pdf â€“ Score: 0.1436 â†’ Poor\n",
            "ðŸ“„ Elizabeth Johnson.pdf â€“ Score: 0.1520 â†’ Poor\n",
            "ðŸ“„ Marcus Fields.pdf â€“ Score: 0.1352 â†’ Poor\n",
            "ðŸ“„ Michael Carter.pdf â€“ Score: 0.1332 â†’ Poor\n",
            "ðŸ“„ Ethan Morales .pdf â€“ Score: 0.1577 â†’ Poor\n",
            "ðŸ“„ David Nguyen.pdf â€“ Score: 0.1100 â†’ Poor\n",
            "ðŸ“„ Amanda Hughes - Copy.pdf â€“ Score: 0.1085 â†’ Poor\n",
            "ðŸ“„ Laura Kim .pdf â€“ Score: 0.1013 â†’ Poor\n",
            "ðŸ“„ Jonathan Lee.pdf â€“ Score: 0.0949 â†’ Poor\n",
            "ðŸ“„ Daniel Evans.pdf â€“ Score: 0.0791 â†’ Poor\n",
            "ðŸ“„ Ethan Clark.pdf â€“ Score: 0.0580 â†’ Poor\n",
            "- Jessica Thompson.pdf: Poor (0.293)\n",
            "- James  Patel .pdf: Poor (0.263)\n",
            "- Maria Lopez.pdf: Poor (0.23)\n",
            "- Carlos Mendoza .pdf: Poor (0.217)\n",
            "- John  Carter .pdf: Poor (0.216)\n",
            "âœ… FAISS index and metadata saved.\n"
          ]
        }
      ],
      "source": [
        "# âœ… Run & Evaluate the Resume Screening System (all in one cell)\n",
        "\n",
        "# âœ… Test if OpenAI GPT-4o integration is working\n",
        "test = ResumeScreeningSystem()\n",
        "reply = test.client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Say hi\"}]\n",
        ")\n",
        "print(\"âœ… GPT-4o Response:\", reply.choices[0].message.content)\n",
        "\n",
        "# âœ… STEP 2: Create an instance of the system\n",
        "system = ResumeScreeningSystem()\n",
        "\n",
        "# âœ… STEP 3: Automatically load PDF resumes from sidebar\n",
        "import io  # Only needs to be imported once\n",
        "\n",
        "\n",
        "resume_folder = \"/content\"  # PDFs should be uploaded in sidebar (left pane)\n",
        "pdf_files = [f for f in os.listdir(resume_folder) if f.endswith(\".pdf\")]\n",
        "\n",
        "if not pdf_files:\n",
        "    print(\"âš ï¸ No PDF resumes found in /content. Please upload them using the left sidebar (Files tab).\")\n",
        "else:\n",
        "    for filename in pdf_files:\n",
        "        path = os.path.join(resume_folder, filename)\n",
        "        print(f\"ðŸ“‚ Processing: {filename} ...\", end=\" \")\n",
        "        try:\n",
        "            with open(path, \"rb\") as f:\n",
        "                file = io.BytesIO(f.read())\n",
        "                text = system.extract_text(file, filename)\n",
        "                if text.strip():\n",
        "                    system.resume_texts[filename] = text\n",
        "                    print(\"âœ… Success\")\n",
        "                else:\n",
        "                    print(\"âš ï¸ Empty content after extraction.\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed: {e}\")\n",
        "\n",
        "\n",
        "# âœ… STEP 4: Load saved FAISS index if available\n",
        "\n",
        "system.build_faiss_index()\n",
        "\n",
        "\n",
        "# âœ… STEP 5: Run evaluation on multiple job descriptions\n",
        "job_list = [\n",
        "    \"AI Engineer with deployment experience\",\n",
        "    \"Data Scientist with Python and ML\",\n",
        "    \"NLP Researcher in Arabic Language\",\n",
        "]\n",
        "\n",
        "# âœ… Add safety check before evaluating\n",
        "if not system.resume_texts:\n",
        "    print(\"âŒ No resumes loaded. Please upload resumes before evaluating.\")\n",
        "else:\n",
        "    system.run_multi_job_evaluation_loop(job_list)\n",
        "\n",
        "# âœ… STEP 6: Save FAISS index after evaluation\n",
        "system.save_faiss_index()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jqtlj_uJiDY"
      },
      "source": [
        " ## ðŸ“¦ LangChain Tools Integration and Agent Initialization Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwA7CRjX15aZ",
        "outputId": "b4fcaac5-d034-42ae-a7ef-45e789c94989"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-6-1876025035.py:275: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(\n",
            "/tmp/ipython-input-6-1876025035.py:282: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
            "/tmp/ipython-input-6-1876025035.py:285: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        }
      ],
      "source": [
        "# âœ… Define tools\n",
        "\n",
        "\n",
        "from langchain.tools import tool\n",
        "\n",
        "# ðŸ“Š Resume Evaluation\n",
        "@tool\n",
        "def evaluate_resumes(job_description: str) -> str:\n",
        "    \"\"\"Evaluate resumes against a job description and return top candidates.\"\"\"\n",
        "    system.job_description = job_description\n",
        "    results = system.evaluate_resumes()\n",
        "    top = sorted(results, key=lambda x: -x['Similarity'])[:5]\n",
        "    return \"\\n\".join([f\"{r['Name']}: {r['Fit']} ({r['Similarity']})\" for r in top])\n",
        "\n",
        "\n",
        "\n",
        "@tool\n",
        "def show_dashboard(dummy: str = \"\") -> str:\n",
        "    \"\"\"Display a bar chart of fit categories from the latest evaluation.\"\"\"\n",
        "    system.show_dashboard()\n",
        "    return \"ðŸ“Š Dashboard displayed.\"\n",
        "\n",
        "\n",
        "\n",
        "@tool\n",
        "def show_highly_recommended(dummy: str = \"\") -> str:\n",
        "    \"\"\"Show resumes that are marked as 'Highly Recommended'.\"\"\"\n",
        "    top = [r for r in system.latest_results if r['Recommendation'] == \"Highly Recommended\"]\n",
        "    return \"\\n\".join([f\"{r['Name']}: {r['Fit']} ({r['Similarity']})\" for r in top])\n",
        "\n",
        "\n",
        "@tool\n",
        "def export_resumes_to_excel(dummy: str = \"\") -> str:\n",
        "    \"\"\"Export current resume evaluation results to Excel.\"\"\"\n",
        "    system.export_resumes_to_excel()\n",
        "    return \"âœ… Excel export completed.\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def export_resumes_to_pdf(dummy: str = \"\") -> str:\n",
        "    \"\"\"Export current resume evaluation results to PDF.\"\"\"\n",
        "    system.export_resumes_to_pdf()\n",
        "    return \"âœ… PDF export completed.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@tool\n",
        "def generate_resume_summary_pdf(tool_input: str) -> str:\n",
        "    \"\"\"Generate a summary PDF for the top N resumes and save it.\"\"\"\n",
        "    cleaned = tool_input.strip()\n",
        "\n",
        "    if not cleaned.isdigit():\n",
        "        return \"âš ï¸ Enter a valid number like '3'.\"\n",
        "\n",
        "    top_n = int(cleaned)\n",
        "    if top_n <= 0:\n",
        "        return \"âš ï¸ Please enter a number greater than 0.\"\n",
        "\n",
        "    # âœ… Call the actual summary PDF generation logic here\n",
        "    # Replace this with your real logic\n",
        "    result = system.generate_summary_pdf_for_top_n(top_n)\n",
        "\n",
        "    return f\"âœ… Summary PDF generated for top {top_n} resumes.\\nðŸ“Ž Check left sidebar (Files tab) to download: {result}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ðŸ“§ Email + Calendar\n",
        "\n",
        "@tool\n",
        "def generate_invite_email(data: str) -> str:\n",
        "    \"\"\"Generate an interview invitation email for a given candidate and job.\"\"\"\n",
        "    try:\n",
        "        name = data.split(\",\")[0].split(\":\")[1].strip()\n",
        "        job = data.split(\",\")[1].split(\":\")[1].strip()\n",
        "        return system.generate_invite_email(name, job)\n",
        "    except:\n",
        "        return \"âš ï¸ Format: name: John, job: Data Scientist\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def schedule_interview(dummy: str = \"\") -> str:\n",
        "    \"\"\"Display interview invitation details manually.\"\"\"\n",
        "    return \"ðŸ“… Copy & paste interview info from system.schedule_interview() manually\"\n",
        "\n",
        "\n",
        "# ðŸ§  GPT-Based Intelligence\n",
        "@tool\n",
        "def generate_job_description(title: str) -> str:\n",
        "    \"\"\"Generate a job description using GPT based on a title.\"\"\"\n",
        "    return system.generate_job_description(title)\n",
        "\n",
        "\n",
        "@tool\n",
        "def rag_resume_similarity_analysis(dummy: str = \"\") -> str:\n",
        "    \"\"\"Find resume pairs with similarity percentage and export to CSV.\"\"\"\n",
        "    try:\n",
        "        results = system.rag_resume_similarity_analysis()\n",
        "\n",
        "        # âœ… Fix ambiguous truth value error\n",
        "        if results is None or len(results) == 0:\n",
        "            return \"âš ï¸ Not enough resumes to compare.\"\n",
        "\n",
        "        import pandas as pd\n",
        "\n",
        "        # âœ… Save results to CSV\n",
        "        df = pd.DataFrame(results, columns=[\"Resume A\", \"Resume B\", \"Similarity (%)\"])\n",
        "        csv_path = \"/content/resume_similarity_results.csv\"\n",
        "        df.to_csv(csv_path, index=False)\n",
        "\n",
        "        # âœ… Format top 10 most similar for CLI\n",
        "        top_matches = results[:10]\n",
        "        formatted = \"\\n\".join([\n",
        "            f\"âœ… {a} â†” {b}: {score}%\" for a, b, score in top_matches\n",
        "        ])\n",
        "\n",
        "        return f\"ðŸ” Top Resume Similarities:\\n{formatted}\\n\\nðŸ“ Full results saved to: {csv_path}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ Error computing similarities: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@tool\n",
        "def detect_missing_skills(tool_input: str = \"\") -> str:\n",
        "    \"\"\"Detect missing skills in each resume based on job and top candidates.\"\"\"\n",
        "    return system.detect_missing_skills()\n",
        "\n",
        "\n",
        "@tool\n",
        "def generate_mock_questions(resume_text: str) -> str:\n",
        "    \"\"\"Generate 5 mock interview questions based on a resume, job description, and top candidates.\"\"\"\n",
        "    return system.generate_mock_questions(resume_text)\n",
        "\n",
        "\n",
        "@tool\n",
        "def rag_resume_improvement(resume_text: str) -> str:\n",
        "    \"\"\"Suggest resume improvements using top candidates as context.\"\"\"\n",
        "    return system.rag_resume_improvement(resume_text)\n",
        "\n",
        "@tool\n",
        "def recommend_roles(resume_text: str) -> str:\n",
        "    \"\"\"Recommend 3 job roles that best fit the resume content.\"\"\"\n",
        "    return system.recommend_roles(resume_text)\n",
        "\n",
        "@tool\n",
        "def extract_skill_profile(resume_text: str) -> str:\n",
        "    \"\"\"Extract technical and soft skills from a resume.\"\"\"\n",
        "    return system.extract_skill_profile(resume_text)\n",
        "\n",
        "# ðŸ” Contact / Metadata / System\n",
        "\n",
        "@tool\n",
        "def extract_contact_links(resume_text: str) -> str:\n",
        "    \"\"\"Extract email, LinkedIn, and GitHub links from a resume.\"\"\"\n",
        "    links = system.extract_contact_links(resume_text)\n",
        "    return f\"âœ‰ï¸ Emails: {links['emails']}\\nðŸ”— LinkedIn: {links['linkedin']}\\nðŸ™ GitHub: {links['github']}\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def reset_system(dummy: str = \"\") -> str:\n",
        "    \"\"\"Reset the system to clear loaded resumes and state.\"\"\"\n",
        "    system.reset_system()\n",
        "    return \"ðŸ” System reset completed.\"\n",
        "\n",
        "\n",
        "\n",
        "@tool\n",
        "def show_system_status(dummy: str = \"\") -> str:\n",
        "    \"\"\"Display internal system status (e.g., embeddings ready, index loaded).\"\"\"\n",
        "    from io import StringIO\n",
        "    output = StringIO()\n",
        "    print(\"âœ… Resumes Loaded:\", len(system.resume_texts), file=output)\n",
        "    print(\"âœ… Job Description Set:\", bool(system.job_description), file=output)\n",
        "    print(\"âœ… FAISS Index:\", bool(system.index), file=output)\n",
        "    print(\"âœ… Embeddings Ready:\", bool(system.resume_embeddings), file=output)\n",
        "    return output.getvalue()\n",
        "\n",
        "\n",
        "\n",
        "@tool\n",
        "def detect_country_from_text(resume_text: str) -> str:\n",
        "    \"\"\"Detect countries mentioned in the resume text.\"\"\"\n",
        "    countries = system.detect_country_from_text(resume_text)\n",
        "    return f\"ðŸŒ Countries Detected: {countries}\" if countries else \"ðŸŒ No country found.\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def auto_alert_recruiter(job_title: str = \"\") -> str:\n",
        "    \"\"\"Trigger an alert with top candidate names for a specific job title.\"\"\"\n",
        "    system.auto_alert_recruiter(job_title=job_title)\n",
        "    return f\"ðŸ“¢ Recruiter alert issued for job: {job_title}\"\n",
        "\n",
        "\n",
        "\n",
        "@tool\n",
        "def rag_query_answering(query: str) -> str:\n",
        "    \"\"\"Ask a natural language question and get an answer based on top resumes.\"\"\"\n",
        "    return system.rag_query_answering(query)\n",
        "\n",
        "@tool\n",
        "def run_multi_job_evaluation(job_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Run resume screening for multiple job descriptions (comma-separated).\n",
        "    Example: \"Data Scientist, NLP Engineer, AI Researcher\"\n",
        "    \"\"\"\n",
        "    job_list = [j.strip() for j in job_text.split(\",\")]\n",
        "    output = system.run_multi_job_evaluation_loop(job_list)\n",
        "    return \"âœ… Multi-job evaluation completed. See printed results above.\"\n",
        "\n",
        "\n",
        "\n",
        "@tool\n",
        "def save_faiss_index(dummy: str = \"\") -> str:\n",
        "    \"\"\"Save the FAISS index and metadata to disk.\"\"\"\n",
        "    system.save_faiss_index()\n",
        "    return \"ðŸ’¾ FAISS index and metadata saved.\"\n",
        "\n",
        "\n",
        "\n",
        "@tool\n",
        "def load_faiss_index(dummy: str = \"\") -> str:\n",
        "    \"\"\"Load a previously saved FAISS index and metadata from disk.\"\"\"\n",
        "    system.load_faiss_index()\n",
        "    return \"ðŸ” FAISS index and metadata loaded.\"\n",
        "\n",
        "\n",
        "\n",
        "@tool\n",
        "def exit(dummy: str = \"\") -> str:\n",
        "    \"\"\"Exit the session.\"\"\"\n",
        "    return \"ðŸ‘‹ Session ended. Type Ctrl+C or close the notebook to fully stop.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tools = [\n",
        "    evaluate_resumes,\n",
        "    rag_query_answering,\n",
        "    show_dashboard,\n",
        "    show_highly_recommended,\n",
        "    export_resumes_to_excel,\n",
        "    export_resumes_to_pdf,\n",
        "    generate_resume_summary_pdf,\n",
        "    generate_invite_email,\n",
        "    schedule_interview,\n",
        "    generate_job_description,\n",
        "    rag_resume_similarity_analysis,\n",
        "    detect_missing_skills,\n",
        "    generate_mock_questions,\n",
        "    rag_resume_improvement,\n",
        "    recommend_roles,\n",
        "    extract_skill_profile,\n",
        "    extract_contact_links,\n",
        "    reset_system,\n",
        "    show_system_status,\n",
        "    detect_country_from_text,\n",
        "    auto_alert_recruiter,\n",
        "    run_multi_job_evaluation,\n",
        "    save_faiss_index,\n",
        "    load_faiss_index,\n",
        "    exit,\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# âœ… Initialize LangChain agent Setup\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0,\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"] )\n",
        "\n",
        "\n",
        "# âœ… Initialize memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# âœ… Initialize the agent with tools, memory, and verbosity\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "\n",
        "# âœ… STEP 1â€“3: Job Evaluation, Recruiter Alert, and Interview Scheduling\n",
        "# helper function used to run a preliminary evaluation pipeline before entering the CLI.\n",
        "def run_initial_pipeline():\n",
        "    # âœ… STEP 1: Enter Job Description and Run Evaluation\n",
        "    system.job_description = input(\"ðŸ“„ Enter the job description:\\n\")\n",
        "    results = system.evaluate_resumes()\n",
        "\n",
        "    # âœ… Show Top Candidates\n",
        "    print(\"\\nðŸ“Š Top Candidates:\")\n",
        "    for r in sorted(results, key=lambda x: -x[\"Similarity\"])[:5]:\n",
        "        print(f\"{r['Name']}: {r['Fit']} ({r['Similarity']})\")\n",
        "\n",
        "    # âœ… STEP 2: Notify Recruiter\n",
        "    system.auto_alert_recruiter(job_title=\"Data Scientist\")\n",
        "\n",
        "    # âœ… STEP 3: Generate Email + Schedule Interview\n",
        "    top_candidate = results[0]\n",
        "    contact_info = system.extract_contact_links(system.resume_texts[top_candidate[\"Name\"]])\n",
        "    candidate_email = contact_info[\"emails\"][0] if contact_info[\"emails\"] else \"placeholder@example.com\"\n",
        "\n",
        "    system.schedule_interview(\n",
        "        candidate_email=candidate_email,\n",
        "        candidate_name=top_candidate[\"Name\"],\n",
        "        job_title=\"Data Scientist\",\n",
        "        date_time_str=\"2025-07-05T15:00:00\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOSLBTeAkQUk"
      },
      "source": [
        "# ðŸ› ï¸ Interactive CLI Agent Interface["
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TJWendSSN2r",
        "outputId": "2ad1ed2a-76a5-4129-e517-9baef5955ff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“„ Enter the job description:\n",
            "data science\n"
          ]
        }
      ],
      "source": [
        "# âœ… FIX: Run initial pipeline in background so CLI doesn't get blocked\n",
        "import threading\n",
        "\n",
        "def safe_run_initial_pipeline():\n",
        "    try:\n",
        "        run_initial_pipeline()\n",
        "        print(\"âœ… Initial pipeline finished.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Pipeline error: {e}\")\n",
        "\n",
        "# ðŸ”„ Run in background\n",
        "threading.Thread(target=safe_run_initial_pipeline, daemon=True).start()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z7f37cyId6E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7yw8Ft4KH4y",
        "outputId": "0cc00000-f00b-4d49-ecec-76a6faaf3bea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸŽ¯ Tool-Based Resume Screening Agent CLI - 25 Features in Order\n",
            "Type the number (1-25) of the command you want to run:\n",
            "\n",
            " 1.  evaluate_resumes\n",
            " 2.  rag_query_answering\n",
            " 3.  show_dashboard\n",
            " 4.  show_highly_recommended\n",
            " 5.  export_resumes_to_excel\n",
            " 6.  export_resumes_to_pdf\n",
            " 7.  generate_resume_summary_pdf\n",
            " 8.  generate_invite_email\n",
            " 9.  schedule_interview\n",
            "10.  generate_job_description\n",
            "11.  rag_resume_similarity_analysis\n",
            "12.  detect_missing_skills\n",
            "13.  generate_mock_questions\n",
            "14.  rag_resume_improvement\n",
            "15.  recommend_roles\n",
            "16.  extract_skill_profile\n",
            "17.  extract_contact_links\n",
            "18.  reset_system\n",
            "19.  show_system_status\n",
            "20.  detect_country_from_text\n",
            "21.  auto_alert_recruiter\n",
            "22.  run_multi_job_evaluation\n",
            "23.  save_faiss_index\n",
            "24.  load_faiss_index\n",
            "25.  exit\n",
            "\n",
            "ðŸ“„ Jessica Thompson.pdf â€“ Score: 0.3671 â†’ Poor\n",
            "ðŸ“„ James  Patel .pdf â€“ Score: 0.3219 â†’ Poor\n",
            "ðŸ“„ Maria Lopez.pdf â€“ Score: 0.3861 â†’ Poor\n",
            "ðŸ“„ John  Carter .pdf â€“ Score: 0.3195 â†’ Poor\n",
            "ðŸ“„ Carlos Mendoza .pdf â€“ Score: 0.3150 â†’ Poor\n",
            "ðŸ“„ Amanda Hughes.pdf â€“ Score: 0.3655 â†’ Poor\n",
            "ðŸ“„ Emily Zhao.pdf â€“ Score: 0.3191 â†’ Poor\n",
            "ðŸ“„ Isaac Roberts .pdf â€“ Score: 0.2900 â†’ Poor\n",
            "ðŸ“„ Elizabeth Johnson.pdf â€“ Score: 0.3433 â†’ Poor\n",
            "ðŸ“„ Marcus Fields.pdf â€“ Score: 0.2733 â†’ Poor\n",
            "ðŸ“„ Michael Carter.pdf â€“ Score: 0.3061 â†’ Poor\n",
            "ðŸ“„ Ethan Morales .pdf â€“ Score: 0.2501 â†’ Poor\n",
            "ðŸ“„ David Nguyen.pdf â€“ Score: 0.3371 â†’ Poor\n",
            "ðŸ“„ Amanda Hughes - Copy.pdf â€“ Score: 0.2864 â†’ Poor\n",
            "ðŸ“„ Laura Kim .pdf â€“ Score: 0.2097 â†’ Poor\n",
            "ðŸ“„ Jonathan Lee.pdf â€“ Score: 0.2306 â†’ Poor\n",
            "ðŸ“„ Daniel Evans.pdf â€“ Score: 0.1939 â†’ Poor\n",
            "ðŸ“„ Ethan Clark.pdf â€“ Score: 0.1268 â†’ Poor\n",
            "\n",
            "ðŸ“Š Top Candidates:\n",
            "Maria Lopez.pdf: Poor (0.386)\n",
            "Jessica Thompson.pdf: Poor (0.367)\n",
            "Amanda Hughes.pdf: Poor (0.365)\n",
            "Elizabeth Johnson.pdf: Poor (0.343)\n",
            "David Nguyen.pdf: Poor (0.337)\n",
            "ðŸ“­ No Highly Recommended resumes found.\n",
            "\n",
            "ðŸ“§ --- Interview Invitation Email ---\n",
            "To: jessica.thompson@insightlytics.com\n",
            "Subject: Interview Invitation for Data Scientist\n",
            "Message:\n",
            "\n",
            "Certainly! Below is a sample interview invitation for Jessica Thompson for a Data Scientist role. You would typically send this as an email, but since it's being referred to as a PDF, you can draft it in a document format and then save or export it as a PDF before sending it to the candidate.\n",
            "\n",
            "---\n",
            "\n",
            "**[Your Company Letterhead/Logo]**\n",
            "\n",
            "[Date]\n",
            "\n",
            "**Jessica Thompson**  \n",
            "[Jessica's Address]  \n",
            "[City, State, Zip Code]\n",
            "\n",
            "Dear Jessica Thompson,\n",
            "\n",
            "We are pleased to inform you that your application for the Data Scientist position at [Company Name] has been shortlisted. After reviewing your resume and qualifications, we believe you might be a suitable fit for our team and would like to invite you for an interview to further discuss your experience and aspirations.\n",
            "\n",
            "**Interview Details:**\n",
            "\n",
            "- **Position:** Data Scientist\n",
            "- **Date:** [Date of Interview]\n",
            "- **Time:** [Time of Interview]\n",
            "- **Location:** [Company Address/Virtual Meeting Link]\n",
            "- **Interviewer(s):** [Name(s) and Title(s) of Interviewer(s)]\n",
            "\n",
            "The interview will last approximately [duration]. Please be prepared to discuss your past projects and experience, as well as how you can contribute to the data initiatives at [Company Name]. We are also eager to answer any questions you may have about the role or our company.\n",
            "\n",
            "For a successful interview, we recommend bringing the following:\n",
            "\n",
            "1. A copy of your resume\n",
            "2. Identification for security purposes\n",
            "3. Any relevant portfolio or project documentation that showcases your skills\n",
            "\n",
            "If the interview is virtual, ensure that you have a stable internet connection and familiarize yourself with the video conferencing platform.\n",
            "\n",
            "Please confirm your availability for the interview by replying to this email or contacting me directly at [Your Phone Number] or [Your Email Address].\n",
            "\n",
            "We look forward to the possibility of welcoming you to our team. Thank you for your interest in [Company Name].\n",
            "\n",
            "Best regards,\n",
            "\n",
            "[Your Full Name]  \n",
            "[Your Job Title]  \n",
            "[Company Name]  \n",
            "[Company Phone Number]  \n",
            "[Company Email Address]\n",
            "\n",
            "---\n",
            "\n",
            "Ensure you've filled in the placeholders with the appropriate details before sending the invitation. If you have any specific requirements or additional information, such as confidentiality agreements or assessments, include those as well.\n",
            "\n",
            "ðŸ“… --- Calendar Event Info ---\n",
            "Event: Interview for Data Scientist\n",
            "Candidate: Jessica Thompson.pdf\n",
            "Date & Time (UTC): 2025-07-05T15:00:00\n",
            "ðŸ“Œ Copy & paste this into your Gmail and Google Calendar manually.\n",
            "âœ… Initial pipeline finished.\n",
            "ðŸ“„ Jessica Thompson.pdf â€“ Score: 0.3671 â†’ Poor\n",
            "ðŸ“„ James  Patel .pdf â€“ Score: 0.3219 â†’ Poor\n",
            "ðŸ“„ Maria Lopez.pdf â€“ Score: 0.3861 â†’ Poor\n",
            "ðŸ“„ John  Carter .pdf â€“ Score: 0.3195 â†’ Poor\n",
            "ðŸ“„ Carlos Mendoza .pdf â€“ Score: 0.3150 â†’ Poor\n",
            "ðŸ“„ Amanda Hughes.pdf â€“ Score: 0.3655 â†’ Poor\n",
            "ðŸ“„ Emily Zhao.pdf â€“ Score: 0.3191 â†’ Poor\n",
            "ðŸ“„ Isaac Roberts .pdf â€“ Score: 0.2900 â†’ Poor\n",
            "ðŸ“„ Elizabeth Johnson.pdf â€“ Score: 0.3433 â†’ Poor\n",
            "ðŸ“„ Marcus Fields.pdf â€“ Score: 0.2733 â†’ Poor\n",
            "ðŸ“„ Michael Carter.pdf â€“ Score: 0.3061 â†’ Poor\n",
            "ðŸ“„ Ethan Morales .pdf â€“ Score: 0.2501 â†’ Poor\n",
            "ðŸ“„ David Nguyen.pdf â€“ Score: 0.3371 â†’ Poor\n",
            "ðŸ“„ Amanda Hughes - Copy.pdf â€“ Score: 0.2864 â†’ Poor\n",
            "ðŸ“„ Laura Kim .pdf â€“ Score: 0.2097 â†’ Poor\n",
            "ðŸ“„ Jonathan Lee.pdf â€“ Score: 0.2306 â†’ Poor\n",
            "ðŸ“„ Daniel Evans.pdf â€“ Score: 0.1939 â†’ Poor\n",
            "ðŸ“„ Ethan Clark.pdf â€“ Score: 0.1268 â†’ Poor\n",
            "Maria Lopez.pdf: Poor (0.386)\n",
            "Jessica Thompson.pdf: Poor (0.367)\n",
            "Amanda Hughes.pdf: Poor (0.365)\n",
            "Elizabeth Johnson.pdf: Poor (0.343)\n",
            "David Nguyen.pdf: Poor (0.337)\n",
            "ðŸŽ¯ Recommending job roles for ALL resumes (PDF table)...\n",
            "âœ… Excel exported: /content/job_role_recommendations.xlsx\n",
            "ðŸ“‚ Check the left sidebar to download.\n",
            "ðŸ“‚ Check the sidebar for job_role_recommendations.pdf\n"
          ]
        }
      ],
      "source": [
        "# === CLI Interface ===\n",
        "\n",
        "print(\"\\nðŸŽ¯ Tool-Based Resume Screening Agent CLI - 25 Features in Order\")\n",
        "print(\"Type the number (1-25) of the command you want to run:\")\n",
        "print(\"\"\"\n",
        " 1.  evaluate_resumes\n",
        " 2.  rag_query_answering\n",
        " 3.  show_dashboard\n",
        " 4.  show_highly_recommended\n",
        " 5.  export_resumes_to_excel\n",
        " 6.  export_resumes_to_pdf\n",
        " 7.  generate_resume_summary_pdf\n",
        " 8.  generate_invite_email\n",
        " 9.  schedule_interview\n",
        "10.  generate_job_description\n",
        "11.  rag_resume_similarity_analysis\n",
        "12.  detect_missing_skills\n",
        "13.  generate_mock_questions\n",
        "14.  rag_resume_improvement\n",
        "15.  recommend_roles\n",
        "16.  extract_skill_profile\n",
        "17.  extract_contact_links\n",
        "18.  reset_system\n",
        "19.  show_system_status\n",
        "20.  detect_country_from_text\n",
        "21.  auto_alert_recruiter\n",
        "22.  run_multi_job_evaluation\n",
        "23.  save_faiss_index\n",
        "24.  load_faiss_index\n",
        "25.  exit\n",
        "\"\"\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        cmd = input(\"\\nðŸ”§ Enter command number (1-25): \").strip()\n",
        "\n",
        "        if cmd == \"1\":\n",
        "            job = input(\"Enter job description:\\n\")\n",
        "            print(evaluate_resumes.run(job))\n",
        "\n",
        "        elif cmd == \"2\":\n",
        "            query = input(\"Enter your query:\\n\")\n",
        "            print(rag_query_answering.run(query))\n",
        "\n",
        "        elif cmd == \"3\":\n",
        "            print(show_dashboard.run(tool_input=\"show me dashboard\"))\n",
        "\n",
        "        elif cmd == \"4\":\n",
        "            print(show_highly_recommended.run(tool_input=\"\"))\n",
        "\n",
        "        elif cmd == \"5\":\n",
        "            print(export_resumes_to_excel.run(tool_input=\"\"))\n",
        "\n",
        "        elif cmd == \"6\":\n",
        "            print(export_resumes_to_pdf.run(tool_input=\"\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        elif cmd == \"7\":\n",
        "            top_n = input(\"How many top resumes? (e.g. 3): \").strip()\n",
        "            print(generate_resume_summary_pdf.run(tool_input=top_n))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        elif cmd == \"8\":\n",
        "            data = input(\"Format: name: John, job: Data Scientist\\n\")\n",
        "            print(generate_invite_email.run(data))\n",
        "\n",
        "        elif cmd == \"9\":\n",
        "            scheduled = 0\n",
        "            for resume in system.latest_results:\n",
        "                if resume[\"Recommendation\"] == \"Highly Recommended\":\n",
        "                    name = resume[\"Name\"]\n",
        "                    email = resume.get(\"Email\", f\"{name.replace(' ', '').lower()}@example.com\")\n",
        "                    job = \"Interview Candidate\"\n",
        "                    time = f\"2025-07-21 {10 + scheduled}:00\"  # Example times: 10:00, 11:00, etc.\n",
        "\n",
        "                    system.schedule_interview(email, name, job, time)\n",
        "                    scheduled += 1\n",
        "\n",
        "            if scheduled == 0:\n",
        "                print(\"âš ï¸ No 'Highly Recommended' candidates found.\")\n",
        "            else:\n",
        "                print(f\"âœ… Scheduled interviews for {scheduled} candidates.\")\n",
        "\n",
        "\n",
        "        elif cmd == \"10\":\n",
        "            title = input(\"Enter job title:\\n\")\n",
        "            print(generate_job_description.run(title))\n",
        "\n",
        "        elif cmd == \"11\":\n",
        "            print(rag_resume_similarity_analysis.run(tool_input=\"\"))\n",
        "\n",
        "\n",
        "\n",
        "        elif cmd == \"12\":\n",
        "                 print(\"ðŸ” Detecting missing skills for each resume based on your entered job...\\n\")\n",
        "                 print(detect_missing_skills.run(tool_input=\"\"))\n",
        "\n",
        "\n",
        "\n",
        "        elif cmd == \"13\":\n",
        "            text = input(\"Paste resume text:\\n\")\n",
        "            print(generate_mock_questions.run(text))\n",
        "\n",
        "\n",
        "\n",
        "        elif cmd == \"14\":\n",
        "             print(rag_resume_improvement.run(tool_input=\"\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        elif cmd == \"15\":\n",
        "             print(\"ðŸŽ¯ Recommending job roles for ALL resumes (PDF table)...\")\n",
        "             print(recommend_roles.run(tool_input=\"\"))\n",
        "             print(\"ðŸ“‚ Check the sidebar for job_role_recommendations.pdf\")\n",
        "\n",
        "        elif cmd == \"16\":\n",
        "            text = input(\"Paste resume text:\\n\")\n",
        "            print(extract_skill_profile.run(text))\n",
        "\n",
        "        elif cmd == \"17\":\n",
        "            text = input(\"Paste resume text:\\n\")\n",
        "            print(extract_contact_links.run(text))\n",
        "\n",
        "        elif cmd == \"18\":\n",
        "            print(reset_system.run(tool_input=\"\"))\n",
        "\n",
        "        elif cmd == \"19\":\n",
        "            print(show_system_status.run(tool_input=\"\"))\n",
        "\n",
        "        elif cmd == \"20\":\n",
        "            text = input(\"Paste resume text:\\n\")\n",
        "            print(detect_country_from_text.run(text))\n",
        "\n",
        "        elif cmd == \"21\":\n",
        "            job_title = input(\"Enter job title:\\n\")\n",
        "            print(auto_alert_recruiter.run(job_title))\n",
        "\n",
        "        elif cmd == \"22\":\n",
        "            jobs = input(\"Comma-separated jobs (e.g. Data Scientist, NLP Engineer):\\n\")\n",
        "            print(run_multi_job_evaluation.run(jobs))\n",
        "\n",
        "        elif cmd == \"23\":\n",
        "            print(save_faiss_index.run(tool_input=\"\"))\n",
        "\n",
        "        elif cmd == \"24\":\n",
        "            print(load_faiss_index.run(tool_input=\"\"))\n",
        "\n",
        "        elif cmd == \"25\":\n",
        "            print(\"ðŸ‘‹ Exiting CLI.\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"âš ï¸ Invalid number. Please enter 1 to 25.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL5BQda9_Bhz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa8ZXtps_BQ4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuYEcptwKHzZ",
        "outputId": "db89ff22-4c52-4c76-960e-6aad231db5b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['Elizabeth Johnson.pdf', 'Daniel Evans.pdf', 'Jonathan Lee.pdf', 'Emily Zhao.pdf', 'Isaac Roberts .pdf', 'Amanda Hughes - Copy.pdf', 'David Nguyen.pdf', 'Emily Zhang .pdf', 'Isabella Torres.pdf', 'Ethan Morales .pdf', 'Carlos Mendoza .pdf', 'John  Carter .pdf', 'James  Patel .pdf', 'Amanda Hughes.pdf', 'Ethan Clark.pdf', 'Kevin Oâ€™Brien .pdf', 'Jessica Thompson.pdf', 'Ava Johnson.pdf'])\n"
          ]
        }
      ],
      "source": [
        "print(system.resume_texts.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR04vImu6CiB",
        "outputId": "51f8b370-d097-409c-b6e6-a0c6f1a1ad69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Total resumes stored: 18\n",
            "âœ… Resume names: ['Elizabeth Johnson.pdf', 'Daniel Evans.pdf', 'Jonathan Lee.pdf', 'Emily Zhao.pdf', 'Isaac Roberts .pdf', 'Amanda Hughes - Copy.pdf', 'David Nguyen.pdf', 'Emily Zhang .pdf', 'Isabella Torres.pdf', 'Ethan Morales .pdf', 'Carlos Mendoza .pdf', 'John  Carter .pdf', 'James  Patel .pdf', 'Amanda Hughes.pdf', 'Ethan Clark.pdf', 'Kevin Oâ€™Brien .pdf', 'Jessica Thompson.pdf', 'Ava Johnson.pdf']\n"
          ]
        }
      ],
      "source": [
        "print(\"âœ… Total resumes stored:\", len(system.resume_texts))\n",
        "print(\"âœ… Resume names:\", list(system.resume_texts.keys()))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}